# Feature-Engineering-Techniques
This repository contains End to End Feature Engineering Techniques for both Numerical and Categorical Data.

**Introduction**
Feature engineering is a very important aspect of machine learning and data science and should never be ignored. The main goal of Feature engineering is to get the best results from the algorithms.

**Table of Contents:**
1. Why should we use Feature Engineering in data science?
2. Handling missing values
3. Feature Selection
4. Handling imbalanced data
5. Handling outliers
6. Binning
7. Encoding
8. Feature Scaling

**1. Why should we use Feature Engineering in data science?**
In Data Science, the performance of the model is depending on data preprocessing and data handling. Suppose if we build a model without Handling data, we got an accuracy of around 70%. By applying the Feature engineering on the same model there is a chance to increase the performance from 70% to more.
Simply, by using Feature Engineering we improve the performance of the model.


**2. Handling missing values**
In some datasets, we got the NA values in features. It is nothing but missing data. By handling this type of data there are many ways:
This is divided into Numerical and Categorical Data Types
